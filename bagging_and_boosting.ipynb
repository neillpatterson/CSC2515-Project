{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import SGDClassifier as LR\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from numpy.random import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load loadData.py\n",
    "\n",
    "def load(synthetic=False):\n",
    "  if (not synthetic):\n",
    "    train = pd.read_csv('train/X_train.txt', header=None, delim_whitespace=True)\n",
    "    test = pd.read_csv('test/X_test.txt', header=None, delim_whitespace=True)\n",
    "\n",
    "    label = pd.read_csv('train/y_train.txt', header=None, delim_whitespace=True)\n",
    "    test_label = pd.read_csv('test/y_test.txt', header=None, delim_whitespace=True)\n",
    "\n",
    "    return train, label, test, test_label\n",
    "\n",
    "  mu1 = np.array([0, 1])\n",
    "  mu2 = np.array([-0.5, -1.0])\n",
    "  mu3 = np.array([3.2, .6])\n",
    "  mu4 = np.array([3, -1])\n",
    "\n",
    "  s1 = np.matrix([[1, 0.1], [0.1, 0.1]])\n",
    "  s2 = np.matrix([[0.3, 0.2], [0.2, 0.4]])\n",
    "  s3 = np.matrix([[0.5, 0.01], [0.01, 0.1]])\n",
    "  s4 = np.matrix([[0.5, -0.2], [-0.2, 0.2]])\n",
    "\n",
    "  d1 = multivariate_normal(mu1, s1, 500)\n",
    "  d2 = multivariate_normal(mu2, s2, 500)\n",
    "  d3 = multivariate_normal(mu3, s3, 500)\n",
    "  d4 = multivariate_normal(mu4, s4, 500)\n",
    "  syntheticTrain = np.vstack((d1,d2,d3,d4))\n",
    "  syntheticTrainLabel = np.hstack(([1] * 500, [2] * 500, [3] * 500, [4] * 500))\n",
    "\n",
    "\n",
    "  t1 = multivariate_normal(mu1, s1, 500)\n",
    "  t2 = multivariate_normal(mu2, s2, 500)\n",
    "  t3 = multivariate_normal(mu3, s3, 500)\n",
    "  t4 = multivariate_normal(mu4, s4, 500)\n",
    "  syntheticTest = np.vstack((t1,t2,t3,t4))\n",
    "  syntheticTestLabel = np.hstack(([1] * 500, [2] * 500, [3] * 500, [4] * 500))\n",
    "\n",
    "  fig, (ax1) = plt.subplots(1,1)\n",
    "\n",
    "  ax1.scatter(d1[:,0], d1[:,1], color='red')\n",
    "  ax1.scatter(d2[:,0], d2[:,1], color='blue')\n",
    "  ax1.scatter(d3[:,0], d3[:,1], color='green')\n",
    "  ax1.scatter(d4[:,0], d4[:,1], color='black')\n",
    "\n",
    "  return syntheticTrain, syntheticTrainLabel, syntheticTest, syntheticTestLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaBoostTrain(trainData, trainLabels, nClassifiers = 20):\n",
    "  nTrain = trainData.shape[0]\n",
    "  alphas = np.zeros(nClassifiers)\n",
    "  dataWeights = np.ones((nTrain, nClassifiers)) / float(nTrain)\n",
    "  models = []\n",
    "\n",
    "  for iteration in xrange(nClassifiers):\n",
    "    #model = LR()\n",
    "    model = DecisionTreeClassifier(max_depth=2)\n",
    "    model.fit(trainData, np.array(trainLabels).ravel(), sample_weight=dataWeights[:, iteration]*nTrain)\n",
    "    models.append(model)\n",
    "    predict = model.predict(trainData)\n",
    "    incorrect = predict != np.array(trainLabels).ravel()\n",
    "    correct = np.invert(incorrect)\n",
    "    weightedErrors = (incorrect) * dataWeights[:,iteration]\n",
    "    errorRate = np.sum(weightedErrors)\n",
    "    alphas[iteration] = np.log((1 - errorRate) / errorRate) + np.log(5)\n",
    "    if (iteration < nClassifiers - 1):\n",
    "      dataWeights[:, iteration + 1] = dataWeights[:, iteration] * np.exp(alphas[iteration] * incorrect)\n",
    "      dataWeights[:, iteration + 1] /= np.sum(dataWeights[:, iteration + 1])\n",
    "\n",
    "  return models, alphas\n",
    "\n",
    "def adaBoostEvaluate(models, alphas, data, labels):\n",
    "  # There are 6 possible classes. We need to keep a tally\n",
    "  # For each class for each data point\n",
    "  tallies = np.zeros((data.shape[0], 6))\n",
    "  for i in xrange(len(alphas)):\n",
    "      modelPredictions = models[i].predict(data)\n",
    "\n",
    "      for j in xrange(len(modelPredictions)):\n",
    "          prediction = modelPredictions[j]\n",
    "          tallies[j, prediction - 1] += alphas[i]\n",
    "\n",
    "  predictions = np.add(np.argmax(tallies, axis=1), 1)\n",
    "  errors = (predictions != np.array(labels).ravel())\n",
    "  return str(100 * np.mean(errors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 5)\n",
      "Training Error: 1.61860718172\n",
      "Test Error: 3.80047505938\n"
     ]
    }
   ],
   "source": [
    "trainData, trainLabels, testData, testLabels = load()\n",
    "\n",
    "lda = LDA(solver=\"svd\", store_covariance=True)\n",
    "#pca = PCA(n_components=3)\n",
    "#X = pca.inverse_transform(pca.fit_transform(trainData))\n",
    "trainData = lda.fit_transform(trainData, trainLabels)\n",
    "print trainData.shape\n",
    "testData = lda.transform(testData)\n",
    "    \n",
    "models, alphas = adaBoostTrain(trainData, trainLabels)\n",
    "\n",
    "trainError = adaBoostEvaluate(models, alphas, trainData, trainLabels)\n",
    "print \"Training Error: \" + trainError\n",
    "\n",
    "testError = adaBoostEvaluate(models, alphas, testData, testLabels)\n",
    "print \"Test Error: \" + testError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData, trainLabels, testData, testLabels = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LR()\n",
    "model.fit(trainData, np.array(trainLabels).ravel())\n",
    "bagging = BaggingClassifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagging.fit(trainData, np.array(trainLabels).ravel())\n",
    "predict_test = bagging.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 5.25958601968\n"
     ]
    }
   ],
   "source": [
    "errors_test = (predict_test != np.array(testLabels).ravel())\n",
    "print \"Test Error: \" + str(100* np.mean(errors_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
