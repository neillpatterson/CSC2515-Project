{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msamuel/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n",
      "/Users/msamuel/Projects/scikit-learn/sklearn/lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/Users/msamuel/Projects/scikit-learn/sklearn/cross_validation.py:42: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "#from sklearn.linear_model import SGDClassifier as LR\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from numpy.random import multivariate_normal\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load loadData.py\n",
    "\n",
    "def load(synthetic=False):\n",
    "  if (not synthetic):\n",
    "    train = pd.read_csv('train/X_train.txt', header=None, delim_whitespace=True)\n",
    "    test = pd.read_csv('test/X_test.txt', header=None, delim_whitespace=True)\n",
    "\n",
    "    label = pd.read_csv('train/y_train.txt', header=None, delim_whitespace=True)\n",
    "    test_label = pd.read_csv('test/y_test.txt', header=None, delim_whitespace=True)\n",
    "\n",
    "    return train, label, test, test_label\n",
    "\n",
    "  mu1 = np.array([0, 1])\n",
    "  mu2 = np.array([-0.5, -1.0])\n",
    "  mu3 = np.array([3.2, .6])\n",
    "  mu4 = np.array([3, -1])\n",
    "\n",
    "  s1 = np.matrix([[1, 0.1], [0.1, 0.1]])\n",
    "  s2 = np.matrix([[0.3, 0.2], [0.2, 0.4]])\n",
    "  s3 = np.matrix([[0.5, 0.01], [0.01, 0.1]])\n",
    "  s4 = np.matrix([[0.5, -0.2], [-0.2, 0.2]])\n",
    "\n",
    "  d1 = multivariate_normal(mu1, s1, 500)\n",
    "  d2 = multivariate_normal(mu2, s2, 500)\n",
    "  d3 = multivariate_normal(mu3, s3, 500)\n",
    "  d4 = multivariate_normal(mu4, s4, 500)\n",
    "  syntheticTrain = np.vstack((d1,d2,d3,d4))\n",
    "  syntheticTrainLabel = np.hstack(([1] * 500, [2] * 500, [3] * 500, [4] * 500))\n",
    "\n",
    "\n",
    "  t1 = multivariate_normal(mu1, s1, 500)\n",
    "  t2 = multivariate_normal(mu2, s2, 500)\n",
    "  t3 = multivariate_normal(mu3, s3, 500)\n",
    "  t4 = multivariate_normal(mu4, s4, 500)\n",
    "  syntheticTest = np.vstack((t1,t2,t3,t4))\n",
    "  syntheticTestLabel = np.hstack(([1] * 500, [2] * 500, [3] * 500, [4] * 500))\n",
    "\n",
    "  fig, (ax1) = plt.subplots(1,1)\n",
    "\n",
    "  ax1.scatter(d1[:,0], d1[:,1], color='red')\n",
    "  ax1.scatter(d2[:,0], d2[:,1], color='blue')\n",
    "  ax1.scatter(d3[:,0], d3[:,1], color='green')\n",
    "  ax1.scatter(d4[:,0], d4[:,1], color='black')\n",
    "\n",
    "  return syntheticTrain, syntheticTrainLabel, syntheticTest, syntheticTestLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaBoostTrain(trainData, trainLabels, nClassifiers = 20):\n",
    "  nTrain = trainData.shape[0]\n",
    "  alphas = np.zeros(nClassifiers)\n",
    "  dataWeights = np.ones((nTrain, nClassifiers)) / float(nTrain)\n",
    "  models = []\n",
    "\n",
    "  for iteration in xrange(nClassifiers):\n",
    "    #model = LR()\n",
    "    model = DecisionTreeClassifier(max_depth=2)\n",
    "    model.fit(trainData, np.array(trainLabels).ravel(), sample_weight=dataWeights[:, iteration]*nTrain)\n",
    "    models.append(model)\n",
    "    predict = model.predict(trainData)\n",
    "    incorrect = predict != np.array(trainLabels).ravel()\n",
    "    correct = np.invert(incorrect)\n",
    "    weightedErrors = (incorrect) * dataWeights[:,iteration]\n",
    "    errorRate = np.sum(weightedErrors)\n",
    "    alphas[iteration] = np.log((1 - errorRate) / errorRate) + np.log(5)\n",
    "    if (iteration < nClassifiers - 1):\n",
    "      dataWeights[:, iteration + 1] = dataWeights[:, iteration] * np.exp(alphas[iteration] * incorrect)\n",
    "      dataWeights[:, iteration + 1] /= np.sum(dataWeights[:, iteration + 1])\n",
    "\n",
    "  return models, alphas\n",
    "\n",
    "def adaBoostEvaluate(models, alphas, data, labels):\n",
    "  # There are 6 possible classes. We need to keep a tally\n",
    "  # For each class for each data point\n",
    "  tallies = np.zeros((data.shape[0], 6))\n",
    "  for i in xrange(len(alphas)):\n",
    "      modelPredictions = models[i].predict(data)\n",
    "\n",
    "      for j in xrange(len(modelPredictions)):\n",
    "          prediction = modelPredictions[j]\n",
    "          tallies[j, prediction - 1] += alphas[i]\n",
    "\n",
    "  predictions = np.add(np.argmax(tallies, axis=1), 1)\n",
    "  errors = (predictions != np.array(labels).ravel())\n",
    "  return str(100 * np.mean(errors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 5)\n",
      "Training Error: 1.61860718172\n",
      "Test Error: 3.80047505938\n"
     ]
    }
   ],
   "source": [
    "trainData, trainLabels, testData, testLabels = load()\n",
    "\n",
    "lda = LDA(solver=\"svd\", store_covariance=True)\n",
    "#pca = PCA(n_components=3)\n",
    "#X = pca.inverse_transform(pca.fit_transform(trainData))\n",
    "trainData = lda.fit_transform(trainData, trainLabels)\n",
    "print trainData.shape\n",
    "testData = lda.transform(testData)\n",
    "    \n",
    "models, alphas = adaBoostTrain(trainData, trainLabels)\n",
    "\n",
    "trainError = adaBoostEvaluate(models, alphas, trainData, trainLabels)\n",
    "print \"Training Error: \" + trainError\n",
    "\n",
    "testError = adaBoostEvaluate(models, alphas, testData, testLabels)\n",
    "print \"Test Error: \" + testError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
